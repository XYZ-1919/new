{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from fe_ import process_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from C:\\Users\\91786\\Desktop\\DS\\food_ds\\data\\extracted\\train.csv\n",
      "Starting feature engineering pipeline...\n",
      "Applying DataCleaner...\n",
      "Applying GeospatialFeatureTransformer...\n",
      "Applying TimeFeatureTransformer...\n",
      "     Adding time period features...\n",
      "Applying DeliveryPersonFeatureTransformer...\n",
      "Applying OrderFeatureTransformer...\n",
      "Applying EngineerTimeFeatures...\n",
      "Applying EngineerDistanceFeatures...\n",
      "Applying EngineerDeliveryFeatures...\n",
      "Feature engineering pipeline completed!\n",
      "Saving processed data to C:\\Users\\91786\\Desktop\\DS\\food_ds\\data\\extracted\\ext.csv\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "input_path = r\"C:\\Users\\91786\\Desktop\\DS\\food_ds\\data\\extracted\\train.csv\" \n",
    "output_path = r\"C:\\Users\\91786\\Desktop\\DS\\food_ds\\data\\extracted\\ext.csv\" \n",
    "df = process_dataset(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                            0\n",
       "Delivery_person_ID            0\n",
       "Delivery_person_Age        1854\n",
       "Delivery_person_Ratings    1908\n",
       "Restaurant_latitude           0\n",
       "                           ... \n",
       "Is_Long_Distance              0\n",
       "Distance_Complexity           0\n",
       "Delivery_Complexity           0\n",
       "Experience_Level              0\n",
       "Delivery_Efficiency           0\n",
       "Length: 95, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_df = pd.read_csv(r\"C:\\Users\\91786\\Desktop\\DS\\food_ds\\data\\extracted\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Delivery_person_ID</th>\n",
       "      <th>Delivery_person_Age</th>\n",
       "      <th>Delivery_person_Ratings</th>\n",
       "      <th>Restaurant_latitude</th>\n",
       "      <th>Restaurant_longitude</th>\n",
       "      <th>Delivery_location_latitude</th>\n",
       "      <th>Delivery_location_longitude</th>\n",
       "      <th>Order_Date</th>\n",
       "      <th>Time_Orderd</th>\n",
       "      <th>Time_Order_picked</th>\n",
       "      <th>Weatherconditions</th>\n",
       "      <th>Road_traffic_density</th>\n",
       "      <th>Vehicle_condition</th>\n",
       "      <th>Type_of_order</th>\n",
       "      <th>Type_of_vehicle</th>\n",
       "      <th>multiple_deliveries</th>\n",
       "      <th>Festival</th>\n",
       "      <th>City</th>\n",
       "      <th>Time_taken(min)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x4607</td>\n",
       "      <td>INDORES13DEL02</td>\n",
       "      <td>37</td>\n",
       "      <td>4.9</td>\n",
       "      <td>22.745049</td>\n",
       "      <td>75.892471</td>\n",
       "      <td>22.765049</td>\n",
       "      <td>75.912471</td>\n",
       "      <td>19-03-2022</td>\n",
       "      <td>11:30:00</td>\n",
       "      <td>11:45:00</td>\n",
       "      <td>conditions Sunny</td>\n",
       "      <td>High</td>\n",
       "      <td>2</td>\n",
       "      <td>Snack</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Urban</td>\n",
       "      <td>(min) 24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0xb379</td>\n",
       "      <td>BANGRES18DEL02</td>\n",
       "      <td>34</td>\n",
       "      <td>4.5</td>\n",
       "      <td>12.913041</td>\n",
       "      <td>77.683237</td>\n",
       "      <td>13.043041</td>\n",
       "      <td>77.813237</td>\n",
       "      <td>25-03-2022</td>\n",
       "      <td>19:45:00</td>\n",
       "      <td>19:50:00</td>\n",
       "      <td>conditions Stormy</td>\n",
       "      <td>Jam</td>\n",
       "      <td>2</td>\n",
       "      <td>Snack</td>\n",
       "      <td>scooter</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Metropolitian</td>\n",
       "      <td>(min) 33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x5d6d</td>\n",
       "      <td>BANGRES19DEL01</td>\n",
       "      <td>23</td>\n",
       "      <td>4.4</td>\n",
       "      <td>12.914264</td>\n",
       "      <td>77.678400</td>\n",
       "      <td>12.924264</td>\n",
       "      <td>77.688400</td>\n",
       "      <td>19-03-2022</td>\n",
       "      <td>08:30:00</td>\n",
       "      <td>08:45:00</td>\n",
       "      <td>conditions Sandstorms</td>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "      <td>Drinks</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Urban</td>\n",
       "      <td>(min) 26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x7a6a</td>\n",
       "      <td>COIMBRES13DEL02</td>\n",
       "      <td>38</td>\n",
       "      <td>4.7</td>\n",
       "      <td>11.003669</td>\n",
       "      <td>76.976494</td>\n",
       "      <td>11.053669</td>\n",
       "      <td>77.026494</td>\n",
       "      <td>05-04-2022</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>18:10:00</td>\n",
       "      <td>conditions Sunny</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0</td>\n",
       "      <td>Buffet</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Metropolitian</td>\n",
       "      <td>(min) 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x70a2</td>\n",
       "      <td>CHENRES12DEL01</td>\n",
       "      <td>32</td>\n",
       "      <td>4.6</td>\n",
       "      <td>12.972793</td>\n",
       "      <td>80.249982</td>\n",
       "      <td>13.012793</td>\n",
       "      <td>80.289982</td>\n",
       "      <td>26-03-2022</td>\n",
       "      <td>13:30:00</td>\n",
       "      <td>13:45:00</td>\n",
       "      <td>conditions Cloudy</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "      <td>Snack</td>\n",
       "      <td>scooter</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Metropolitian</td>\n",
       "      <td>(min) 30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45588</th>\n",
       "      <td>0x7c09</td>\n",
       "      <td>JAPRES04DEL01</td>\n",
       "      <td>30</td>\n",
       "      <td>4.8</td>\n",
       "      <td>26.902328</td>\n",
       "      <td>75.794257</td>\n",
       "      <td>26.912328</td>\n",
       "      <td>75.804257</td>\n",
       "      <td>24-03-2022</td>\n",
       "      <td>11:35:00</td>\n",
       "      <td>11:45:00</td>\n",
       "      <td>conditions Windy</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "      <td>Meal</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Metropolitian</td>\n",
       "      <td>(min) 32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45589</th>\n",
       "      <td>0xd641</td>\n",
       "      <td>AGRRES16DEL01</td>\n",
       "      <td>21</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>16-02-2022</td>\n",
       "      <td>19:55:00</td>\n",
       "      <td>20:10:00</td>\n",
       "      <td>conditions Windy</td>\n",
       "      <td>Jam</td>\n",
       "      <td>0</td>\n",
       "      <td>Buffet</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Metropolitian</td>\n",
       "      <td>(min) 36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45590</th>\n",
       "      <td>0x4f8d</td>\n",
       "      <td>CHENRES08DEL03</td>\n",
       "      <td>30</td>\n",
       "      <td>4.9</td>\n",
       "      <td>13.022394</td>\n",
       "      <td>80.242439</td>\n",
       "      <td>13.052394</td>\n",
       "      <td>80.272439</td>\n",
       "      <td>11-03-2022</td>\n",
       "      <td>23:50:00</td>\n",
       "      <td>00:05:00</td>\n",
       "      <td>conditions Cloudy</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>Drinks</td>\n",
       "      <td>scooter</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Metropolitian</td>\n",
       "      <td>(min) 16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45591</th>\n",
       "      <td>0x5eee</td>\n",
       "      <td>COIMBRES11DEL01</td>\n",
       "      <td>20</td>\n",
       "      <td>4.7</td>\n",
       "      <td>11.001753</td>\n",
       "      <td>76.986241</td>\n",
       "      <td>11.041753</td>\n",
       "      <td>77.026241</td>\n",
       "      <td>07-03-2022</td>\n",
       "      <td>13:35:00</td>\n",
       "      <td>13:40:00</td>\n",
       "      <td>conditions Cloudy</td>\n",
       "      <td>High</td>\n",
       "      <td>0</td>\n",
       "      <td>Snack</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Metropolitian</td>\n",
       "      <td>(min) 26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45592</th>\n",
       "      <td>0x5fb2</td>\n",
       "      <td>RANCHIRES09DEL02</td>\n",
       "      <td>23</td>\n",
       "      <td>4.9</td>\n",
       "      <td>23.351058</td>\n",
       "      <td>85.325731</td>\n",
       "      <td>23.431058</td>\n",
       "      <td>85.405731</td>\n",
       "      <td>02-03-2022</td>\n",
       "      <td>17:10:00</td>\n",
       "      <td>17:15:00</td>\n",
       "      <td>conditions Fog</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2</td>\n",
       "      <td>Snack</td>\n",
       "      <td>scooter</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Metropolitian</td>\n",
       "      <td>(min) 36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45593 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID Delivery_person_ID Delivery_person_Age Delivery_person_Ratings  \\\n",
       "0      0x4607     INDORES13DEL02                   37                     4.9   \n",
       "1      0xb379     BANGRES18DEL02                   34                     4.5   \n",
       "2      0x5d6d     BANGRES19DEL01                   23                     4.4   \n",
       "3      0x7a6a    COIMBRES13DEL02                   38                     4.7   \n",
       "4      0x70a2     CHENRES12DEL01                   32                     4.6   \n",
       "...        ...                ...                 ...                     ...   \n",
       "45588  0x7c09      JAPRES04DEL01                   30                     4.8   \n",
       "45589  0xd641      AGRRES16DEL01                   21                     4.6   \n",
       "45590  0x4f8d     CHENRES08DEL03                   30                     4.9   \n",
       "45591  0x5eee    COIMBRES11DEL01                   20                     4.7   \n",
       "45592  0x5fb2   RANCHIRES09DEL02                   23                     4.9   \n",
       "\n",
       "       Restaurant_latitude  Restaurant_longitude  Delivery_location_latitude  \\\n",
       "0                22.745049             75.892471                   22.765049   \n",
       "1                12.913041             77.683237                   13.043041   \n",
       "2                12.914264             77.678400                   12.924264   \n",
       "3                11.003669             76.976494                   11.053669   \n",
       "4                12.972793             80.249982                   13.012793   \n",
       "...                    ...                   ...                         ...   \n",
       "45588            26.902328             75.794257                   26.912328   \n",
       "45589             0.000000              0.000000                    0.070000   \n",
       "45590            13.022394             80.242439                   13.052394   \n",
       "45591            11.001753             76.986241                   11.041753   \n",
       "45592            23.351058             85.325731                   23.431058   \n",
       "\n",
       "       Delivery_location_longitude  Order_Date Time_Orderd Time_Order_picked  \\\n",
       "0                        75.912471  19-03-2022    11:30:00          11:45:00   \n",
       "1                        77.813237  25-03-2022    19:45:00          19:50:00   \n",
       "2                        77.688400  19-03-2022    08:30:00          08:45:00   \n",
       "3                        77.026494  05-04-2022    18:00:00          18:10:00   \n",
       "4                        80.289982  26-03-2022    13:30:00          13:45:00   \n",
       "...                            ...         ...         ...               ...   \n",
       "45588                    75.804257  24-03-2022    11:35:00          11:45:00   \n",
       "45589                     0.070000  16-02-2022    19:55:00          20:10:00   \n",
       "45590                    80.272439  11-03-2022    23:50:00          00:05:00   \n",
       "45591                    77.026241  07-03-2022    13:35:00          13:40:00   \n",
       "45592                    85.405731  02-03-2022    17:10:00          17:15:00   \n",
       "\n",
       "           Weatherconditions Road_traffic_density  Vehicle_condition  \\\n",
       "0           conditions Sunny                High                   2   \n",
       "1          conditions Stormy                 Jam                   2   \n",
       "2      conditions Sandstorms                 Low                   0   \n",
       "3           conditions Sunny              Medium                   0   \n",
       "4          conditions Cloudy                High                   1   \n",
       "...                      ...                  ...                ...   \n",
       "45588       conditions Windy                High                   1   \n",
       "45589       conditions Windy                 Jam                   0   \n",
       "45590      conditions Cloudy                 Low                   1   \n",
       "45591      conditions Cloudy                High                   0   \n",
       "45592         conditions Fog              Medium                   2   \n",
       "\n",
       "      Type_of_order Type_of_vehicle multiple_deliveries Festival  \\\n",
       "0            Snack      motorcycle                    0      No    \n",
       "1            Snack         scooter                    1      No    \n",
       "2           Drinks      motorcycle                    1      No    \n",
       "3           Buffet      motorcycle                    1      No    \n",
       "4            Snack         scooter                    1      No    \n",
       "...             ...             ...                 ...      ...   \n",
       "45588         Meal      motorcycle                    0      No    \n",
       "45589       Buffet      motorcycle                    1      No    \n",
       "45590       Drinks         scooter                    0      No    \n",
       "45591        Snack      motorcycle                    1      No    \n",
       "45592        Snack         scooter                    1      No    \n",
       "\n",
       "                 City Time_taken(min)  \n",
       "0              Urban         (min) 24  \n",
       "1      Metropolitian         (min) 33  \n",
       "2              Urban         (min) 26  \n",
       "3      Metropolitian         (min) 21  \n",
       "4      Metropolitian         (min) 30  \n",
       "...               ...             ...  \n",
       "45588  Metropolitian         (min) 32  \n",
       "45589  Metropolitian         (min) 36  \n",
       "45590  Metropolitian         (min) 16  \n",
       "45591  Metropolitian         (min) 26  \n",
       "45592  Metropolitian         (min) 36  \n",
       "\n",
       "[45593 rows x 20 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 unique city codes:\n",
      "JAP: 3443 deliveries\n",
      "RANCHI: 3229 deliveries\n",
      "BANG: 3195 deliveries\n",
      "SUR: 3187 deliveries\n",
      "HYD: 3181 deliveries\n",
      "MUM: 3173 deliveries\n",
      "MYS: 3171 deliveries\n",
      "COIMB: 3170 deliveries\n",
      "VAD: 3166 deliveries\n",
      "INDO: 3159 deliveries\n",
      "CHEN: 3145 deliveries\n",
      "PUNE: 3132 deliveries\n",
      "AGR: 763 deliveries\n",
      "LUDH: 758 deliveries\n",
      "KNP: 740 deliveries\n",
      "ALH: 740 deliveries\n",
      "DEH: 737 deliveries\n",
      "GOA: 709 deliveries\n",
      "AURG: 703 deliveries\n",
      "KOC: 701 deliveries\n",
      "KOL: 700 deliveries\n",
      "BHP: 691 deliveries\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "# Create a dictionary to count city occurrences\n",
    "city_counts = {}\n",
    "\n",
    "# Extract city codes from Delivery_person_ID\n",
    "for text in og_df['Delivery_person_ID']:\n",
    "    # Find the pattern: [any characters]RES[any characters]\n",
    "    match = re.search(r'([A-Z]+)RES', text)\n",
    "    if match:\n",
    "        city_code = match.group(1)\n",
    "        city_counts[city_code] = city_counts.get(city_code, 0) + 1\n",
    "\n",
    "# Sort cities by frequency and display results\n",
    "sorted_cities = sorted(city_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f\"Found {len(city_counts)} unique city codes:\")\n",
    "for city, count in sorted_cities:\n",
    "    print(f\"{city}: {count} deliveries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45593 entries, 0 to 45592\n",
      "Data columns (total 20 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   ID                           45593 non-null  object \n",
      " 1   Delivery_person_ID           45593 non-null  object \n",
      " 2   Delivery_person_Age          45593 non-null  object \n",
      " 3   Delivery_person_Ratings      45593 non-null  object \n",
      " 4   Restaurant_latitude          45593 non-null  float64\n",
      " 5   Restaurant_longitude         45593 non-null  float64\n",
      " 6   Delivery_location_latitude   45593 non-null  float64\n",
      " 7   Delivery_location_longitude  45593 non-null  float64\n",
      " 8   Order_Date                   45593 non-null  object \n",
      " 9   Time_Orderd                  45593 non-null  object \n",
      " 10  Time_Order_picked            45593 non-null  object \n",
      " 11  Weatherconditions            45593 non-null  object \n",
      " 12  Road_traffic_density         45593 non-null  object \n",
      " 13  Vehicle_condition            45593 non-null  int64  \n",
      " 14  Type_of_order                45593 non-null  object \n",
      " 15  Type_of_vehicle              45593 non-null  object \n",
      " 16  multiple_deliveries          45593 non-null  object \n",
      " 17  Festival                     45593 non-null  object \n",
      " 18  City                         45593 non-null  object \n",
      " 19  Time_taken(min)              45593 non-null  object \n",
      "dtypes: float64(4), int64(1), object(15)\n",
      "memory usage: 7.0+ MB\n"
     ]
    }
   ],
   "source": [
    "og_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                             0\n",
       "Delivery_person_ID             0\n",
       "Delivery_person_Age            0\n",
       "Delivery_person_Ratings        0\n",
       "Restaurant_latitude            0\n",
       "Restaurant_longitude           0\n",
       "Delivery_location_latitude     0\n",
       "Delivery_location_longitude    0\n",
       "Order_Date                     0\n",
       "Time_Orderd                    0\n",
       "Time_Order_picked              0\n",
       "Weatherconditions              0\n",
       "Road_traffic_density           0\n",
       "Vehicle_condition              0\n",
       "Type_of_order                  0\n",
       "Type_of_vehicle                0\n",
       "multiple_deliveries            0\n",
       "Festival                       0\n",
       "City                           0\n",
       "Time_taken(min)                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Time_Orderd'] = pd.to_datetime(df['Time_Orderd'], errors='coerce')\n",
    "df['Time_Order_picked'] = pd.to_datetime(df['Time_Order_picked'], errors='coerce')\n",
    "df['Delivery_person_Age'] = pd.to_numeric(df['Delivery_person_Age'], errors='coerce')\n",
    "df['Delivery_person_Ratings'] = pd.to_numeric(df['Delivery_person_Ratings'], errors='coerce')\n",
    "df['Time_taken(min)'] = pd.to_numeric(df['Time_taken(min)'].str.extract('(\\d+)')[0], errors='coerce')\n",
    "df['multiple_deliveries'] = pd.to_numeric(df['multiple_deliveries'], errors='coerce')\n",
    "df['Order_Date'] = pd.to_datetime(df['Order_Date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('NaN',np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\91786\\Desktop\\DS\\food_ds\\data\\extracted\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import holidays\n",
    "import re\n",
    "from geopy.distance import geodesic\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings # To suppress specific warnings if needed\n",
    "\n",
    "# Suppress specific warnings if they become noisy, e.g., from geopy/holidays\n",
    "# warnings.filterwarnings(\"ignore\", category=SomeWarningCategory)\n",
    "\n",
    "class FeatureTransformer(ABC):\n",
    "    \"\"\"Base abstract class for all feature transformers\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, df):\n",
    "        \"\"\"Transform the dataframe and return a new dataframe with additional features\"\"\"\n",
    "        pass\n",
    "\n",
    "    def _copy_dataframe(self, df):\n",
    "        \"\"\"Create a copy of the dataframe to avoid modifying the original\"\"\"\n",
    "        return df.copy()\n",
    "\n",
    "\n",
    "class DataCleaner(FeatureTransformer):\n",
    "    \"\"\"Handles basic data cleaning, type conversions, and initial string processing.\"\"\"\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = self._copy_dataframe(df)\n",
    "        print(\" -> Running DataCleaner...\")\n",
    "\n",
    "        # Convert relevant columns to numeric, coercing errors to NaN\n",
    "        # NaN imputation will be handled in the Imputer step\n",
    "        df['Delivery_person_Age'] = pd.to_numeric(df['Delivery_person_Age'], errors='coerce')\n",
    "        df['Delivery_person_Ratings'] = pd.to_numeric(df['Delivery_person_Ratings'], errors='coerce')\n",
    "        df['multiple_deliveries'] = pd.to_numeric(df['multiple_deliveries'], errors='coerce')\n",
    "        # Assuming Vehicle_condition is numeric (as per initial info)\n",
    "        df['Vehicle_condition'] = pd.to_numeric(df['Vehicle_condition'], errors='coerce')\n",
    "\n",
    "        # Clean Time_taken(min) - extract the numeric value if it's string\n",
    "        # Check if column exists and is not already numeric\n",
    "        if 'Time_taken(min)' in df.columns and not pd.api.types.is_numeric_dtype(df['Time_taken(min)']):\n",
    "            print(\"     Cleaning Time_taken(min)...\")\n",
    "            # Handle potential formats like '(min) 30' or '30'\n",
    "            df['Time_taken(min)'] = df['Time_taken(min)'].astype(str).str.extract(r'(\\d+)').astype(float)\n",
    "        elif 'Time_taken(min)' in df.columns:\n",
    "             # Ensure it's float even if it was integer\n",
    "             df['Time_taken(min)'] = df['Time_taken(min)'].astype(float)\n",
    "\n",
    "\n",
    "        # Basic Weather conditions cleaning - remove potential prefix\n",
    "        # More complex variations handled by get_dummies later\n",
    "        if 'Weatherconditions' in df.columns:\n",
    "             # Check if it's object type first\n",
    "             if pd.api.types.is_object_dtype(df['Weatherconditions']):\n",
    "                print(\"     Cleaning Weatherconditions prefix...\")\n",
    "                df['Weatherconditions'] = df['Weatherconditions'].str.replace('conditions ', '', regex=False)\n",
    "\n",
    "        # Convert Order_Date to datetime\n",
    "        # Format '%d-%m-%Y' specified, handle potential errors\n",
    "        df['Order_Date'] = pd.to_datetime(df['Order_Date'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "        # Clean Festival column\n",
    "        if 'Festival' in df.columns and pd.api.types.is_object_dtype(df['Festival']):\n",
    "            print(\"     Cleaning Festival column...\")\n",
    "            # Convert to lowercase, strip whitespace, check if 'yes', fill NaN with False\n",
    "            df['Festival_Indicator'] = (df['Festival'].str.strip().str.lower() == 'yes').fillna(False)\n",
    "        elif 'Festival' in df.columns:\n",
    "             # If already boolean or numeric, ensure it's 0/1\n",
    "             df['Festival_Indicator'] = (df['Festival'] == 'Yes') | (df['Festival'] == 1) # Example check\n",
    "             df['Festival_Indicator'] = df['Festival_Indicator'].fillna(False).astype(int)\n",
    "        else:\n",
    "             df['Festival_Indicator'] = 0 # Assume no festival if column missing\n",
    "\n",
    "        # Standardize time fields (assuming they are objects/strings initially)\n",
    "        # This needs to happen before TimeFeatureTransformer combines them\n",
    "        print(\"     Standardizing time strings (Time_Orderd, Time_Order_picked)...\")\n",
    "        for col in ['Time_Orderd', 'Time_Order_picked']:\n",
    "             if col in df.columns and not pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "                 # Handle various time formats and standardize\n",
    "                 df[col] = df[col].astype(str).fillna('NaN') # Keep NaN distinct\n",
    "                 # Extract HH:MM portion from various formats\n",
    "                 df[col] = df[col].apply(lambda x: re.search(r'(\\d{1,2}:\\d{2})', str(x)).group(1)\n",
    "                                           if re.search(r'(\\d{1,2}:\\d{2})', str(x)) else None) # None if no match\n",
    "                 # Add leading zero if needed (handle None)\n",
    "                 df[col] = df[col].apply(lambda x: f\"0{x}\" if x and len(x) < 5 else x)\n",
    "                 # Convert missing/unparseable times back to None/NaN for pd.to_datetime coercion\n",
    "                 df[col] = df[col].replace('None', None)\n",
    "\n",
    "\n",
    "        print(\" -> DataCleaner finished.\")\n",
    "        return df\n",
    "\n",
    "class MissingValueImputer(FeatureTransformer):\n",
    "    \"\"\"Handles imputation of missing values using suitable strategies.\"\"\"\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = self._copy_dataframe(df)\n",
    "        print(\" -> Running MissingValueImputer...\")\n",
    "\n",
    "        # Impute Age with Median\n",
    "        if 'Delivery_person_Age' in df.columns:\n",
    "            median_age = df['Delivery_person_Age'].median()\n",
    "            print(f\"     Imputing Delivery_person_Age NaN with median: {median_age:.2f}\")\n",
    "            df['Delivery_person_Age'].fillna(median_age, inplace=True)\n",
    "\n",
    "        # Impute Ratings with Median\n",
    "        if 'Delivery_person_Ratings' in df.columns:\n",
    "            median_rating = df['Delivery_person_Ratings'].median()\n",
    "            print(f\"     Imputing Delivery_person_Ratings NaN with median: {median_rating:.2f}\")\n",
    "            df['Delivery_person_Ratings'].fillna(median_rating, inplace=True)\n",
    "\n",
    "        # Impute multiple_deliveries with 0 (assuming NaN means 0) and convert to int\n",
    "        if 'multiple_deliveries' in df.columns:\n",
    "            print(\"     Imputing multiple_deliveries NaN with 0 (assuming NaN means none)\")\n",
    "            df['multiple_deliveries'].fillna(0, inplace=True)\n",
    "            df['multiple_deliveries'] = df['multiple_deliveries'].astype(int) # Convert to Int after filling\n",
    "\n",
    "        # Impute Road_traffic_density with Mode (most frequent)\n",
    "        if 'Road_traffic_density' in df.columns and df['Road_traffic_density'].isnull().any():\n",
    "             # Ensure calculation is done on object type if not already imputed\n",
    "             if pd.api.types.is_object_dtype(df['Road_traffic_density']):\n",
    "                 mode_traffic = df['Road_traffic_density'].mode()\n",
    "                 if not mode_traffic.empty:\n",
    "                     mode_traffic_val = mode_traffic[0]\n",
    "                     print(f\"     Imputing Road_traffic_density NaN with mode: '{mode_traffic_val}'\")\n",
    "                     df['Road_traffic_density'].fillna(mode_traffic_val, inplace=True)\n",
    "                 else:\n",
    "                     print(\"     Warning: Could not determine mode for Road_traffic_density. Leaving NaNs.\")\n",
    "\n",
    "\n",
    "        # Note: Time_Orderd NaNs are handled by letting pd.to_datetime create NaT.\n",
    "        # We will impute derived features like Preparation_Time_Mins later if needed.\n",
    "\n",
    "        print(\" -> MissingValueImputer finished.\")\n",
    "        return df\n",
    "\n",
    "\n",
    "class GeospatialFeatureTransformer(FeatureTransformer):\n",
    "    \"\"\"Creates features based on geospatial data\"\"\"\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = self._copy_dataframe(df)\n",
    "        print(\" -> Running GeospatialFeatureTransformer...\")\n",
    "\n",
    "        # Calculate delivery distance (km)\n",
    "        print(\"     Calculating Distance_km...\")\n",
    "        df['Distance_km'] = df.apply(self._calculate_distance, axis=1)\n",
    "\n",
    "        # Impute potential NaNs in Distance_km (e.g., if lat/lon were invalid) with median\n",
    "        if df['Distance_km'].isnull().any():\n",
    "            median_distance = df['Distance_km'].median()\n",
    "            print(f\"     Imputing Distance_km NaN with median: {median_distance:.2f} km\")\n",
    "            df['Distance_km'].fillna(median_distance, inplace=True)\n",
    "\n",
    "        # Note: Redundant City extraction removed. Handled in DeliveryPerson transformer.\n",
    "        print(\" -> GeospatialFeatureTransformer finished.\")\n",
    "        return df\n",
    "\n",
    "    def _calculate_distance(self, row):\n",
    "        \"\"\"Calculate geodesic distance between restaurant and delivery location\"\"\"\n",
    "        try:\n",
    "            # Ensure lat/lon are valid numbers before calculation\n",
    "            lat1, lon1 = float(row['Restaurant_latitude']), float(row['Restaurant_longitude'])\n",
    "            lat2, lon2 = float(row['Delivery_location_latitude']), float(row['Delivery_location_longitude'])\n",
    "            restaurant = (lat1, lon1)\n",
    "            delivery = (lat2, lon2)\n",
    "            return geodesic(restaurant, delivery).km\n",
    "        except (ValueError, TypeError, KeyError) as e:\n",
    "            # Return NaN if coordinates are missing, not numbers, or columns absent\n",
    "            # print(f\"     Warning: Could not calculate distance for row. Error: {e}. Row: {row}\") # Optional: Log errors\n",
    "            return np.nan\n",
    "\n",
    "\n",
    "class TimeFeatureTransformer(FeatureTransformer):\n",
    "    \"\"\"Creates time-based features from datetime fields\"\"\"\n",
    "\n",
    "    def __init__(self, holiday_years=None):\n",
    "        # Using a recent range including typical years found in datasets\n",
    "        self.holiday_years = holiday_years or list(range(datetime.now().year - 3, datetime.now().year + 1))\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = self._copy_dataframe(df)\n",
    "        print(\" -> Running TimeFeatureTransformer...\")\n",
    "\n",
    "        # Order_Date should be datetime from DataCleaner\n",
    "        # Standardized time strings should exist from DataCleaner\n",
    "\n",
    "        # Create datetime objects\n",
    "        df = self._create_datetime_objects(df)\n",
    "\n",
    "        # Extract time components (handle NaT gracefully)\n",
    "        df = self._extract_time_components(df)\n",
    "\n",
    "        # Add time period categorization\n",
    "        df = self._add_time_periods(df)\n",
    "\n",
    "        # Add cyclic encoding\n",
    "        df = self._add_cyclic_encodings(df)\n",
    "\n",
    "        # Add calendar features\n",
    "        df = self._add_calendar_features(df)\n",
    "\n",
    "        # Add holiday features\n",
    "        df = self._add_holiday_features(df)\n",
    "\n",
    "        # Add interaction features\n",
    "        df = self._add_interaction_features(df)\n",
    "\n",
    "        # Impute Preparation Time NaNs (created if Order_Datetime or Pickup_Datetime was NaT)\n",
    "        if 'Preparation_Time_Mins' in df.columns and df['Preparation_Time_Mins'].isnull().any():\n",
    "            median_prep_time = df['Preparation_Time_Mins'].median()\n",
    "            print(f\"     Imputing Preparation_Time_Mins NaN with median: {median_prep_time:.2f} mins\")\n",
    "            df['Preparation_Time_Mins'].fillna(median_prep_time, inplace=True)\n",
    "\n",
    "        # Note: NaNs in Hour, DayOfWeek etc derived from NaT Order_Datetime\n",
    "        # remain NaN. Depending on the model, further imputation (e.g., with mode)\n",
    "        # or using models that handle NaNs might be needed for these derived features.\n",
    "\n",
    "        print(\" -> TimeFeatureTransformer finished.\")\n",
    "        return df\n",
    "\n",
    "    def _create_datetime_objects(self, df):\n",
    "        \"\"\"Create full datetime objects by combining date and time\"\"\"\n",
    "        print(\"     Creating datetime objects (Order_Datetime, Pickup_Datetime)...\")\n",
    "        if 'Order_Date' not in df.columns or df['Order_Date'].isnull().all():\n",
    "             print(\"     Warning: Order_Date column missing or all null. Cannot create datetime objects.\")\n",
    "             df['Order_Datetime'] = pd.NaT\n",
    "             df['Pickup_Datetime'] = pd.NaT\n",
    "             df['Preparation_Time_Mins'] = np.nan\n",
    "             return df\n",
    "\n",
    "        # Ensure Order_Date is string for concatenation if needed, handle NaT\n",
    "        date_str = df['Order_Date'].dt.strftime('%Y-%m-%d').fillna('NaT')\n",
    "\n",
    "        # Combine date and standardized time strings, coerce errors to NaT\n",
    "        df['Order_Datetime'] = pd.to_datetime(\n",
    "             date_str + ' ' + df['Time_Orderd'], errors='coerce'\n",
    "        )\n",
    "        df['Pickup_Datetime'] = pd.to_datetime(\n",
    "             date_str + ' ' + df['Time_Order_picked'], errors='coerce'\n",
    "        )\n",
    "\n",
    "        # Handle midnight crossover (only if both datetimes are valid)\n",
    "        valid_times = df['Order_Datetime'].notna() & df['Pickup_Datetime'].notna()\n",
    "        midnight_crossover = valid_times & (df['Pickup_Datetime'] < df['Order_Datetime'])\n",
    "        df.loc[midnight_crossover, 'Pickup_Datetime'] = df.loc[midnight_crossover, 'Pickup_Datetime'] + pd.Timedelta(days=1)\n",
    "\n",
    "        # Calculate preparation time in minutes (result is NaN if either datetime is NaT)\n",
    "        df['Preparation_Time_Mins'] = (df['Pickup_Datetime'] - df['Order_Datetime']).dt.total_seconds() / 60\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _extract_time_components(self, df):\n",
    "        \"\"\"Extract various time components as features. Handles NaT.\"\"\"\n",
    "        print(\"     Extracting time components (Hour, DayOfWeek, etc.)...\")\n",
    "        dt_col = df['Order_Datetime']\n",
    "        df['Hour'] = dt_col.dt.hour.astype('Int64') # Use nullable integer type\n",
    "        df['Minute'] = dt_col.dt.minute.astype('Int64')\n",
    "        df['Day'] = dt_col.dt.day.astype('Int64')\n",
    "        df['Month'] = dt_col.dt.month.astype('Int64')\n",
    "        df['Year'] = dt_col.dt.year.astype('Int64')\n",
    "        df['DayOfWeek'] = dt_col.dt.dayofweek.astype('Int64') # Monday=0, Sunday=6\n",
    "        df['DayOfYear'] = dt_col.dt.dayofyear.astype('Int64')\n",
    "        df['Quarter'] = dt_col.dt.quarter.astype('Int64')\n",
    "        # isocalendar().week returns object, needs conversion and handling NaT\n",
    "        df['WeekOfYear'] = dt_col.dt.isocalendar().week.astype('Int64')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _add_time_periods(self, df):\n",
    "        \"\"\"Add categorical time periods and rush hour indicators. Handles NaN Hour.\"\"\"\n",
    "        print(\"     Adding time period features...\")\n",
    "        # Time period categorization\n",
    "        # Use Int64 Hour for comparison, pd.cut handles NaN input\n",
    "        time_bins = [-1, 5, 9, 11, 13, 16, 19, 22, 23] # Adjusted bins slightly\n",
    "        time_labels = ['Late_Night', 'Early_Morning', 'Late_Morning', 'Lunch',\n",
    "                       'Afternoon', 'Evening', 'Dinner', 'Night']\n",
    "        df['Time_Period'] = pd.cut(df['Hour'], bins=time_bins, labels=time_labels, right=True)\n",
    "\n",
    "        # Special time indicators (Handle potential NaN from Hour)\n",
    "        df['Is_Weekend'] = df['DayOfWeek'].isin([5, 6]).astype(int) # Sat=5, Sun=6\n",
    "        df['Is_Lunch_Rush'] = df['Hour'].between(11, 13, inclusive='both').astype(int)\n",
    "        df['Is_Dinner_Rush'] = df['Hour'].between(18, 21, inclusive='both').astype(int) # Extended dinner rush\n",
    "        df['Is_Rush_Hour'] = df[['Is_Lunch_Rush', 'Is_Dinner_Rush']].max(axis=1)\n",
    "\n",
    "        # Binary features for meal types\n",
    "        df['Is_Breakfast_Order'] = df['Hour'].between(7, 10, inclusive='both').astype(int)\n",
    "        # Check for NaN before comparison\n",
    "        df['Is_Late_Night_Order'] = (((df['Hour'] >= 22) | (df['Hour'] <= 4)) & df['Hour'].notna()).astype(int)\n",
    "\n",
    "        # Fill NaN results in boolean columns with 0 (conservative assumption)\n",
    "        bool_cols = ['Is_Weekend', 'Is_Lunch_Rush', 'Is_Dinner_Rush', 'Is_Rush_Hour', 'Is_Breakfast_Order', 'Is_Late_Night_Order']\n",
    "        for col in bool_cols:\n",
    "             df[col] = df[col].fillna(0).astype(int)\n",
    "\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _add_cyclic_encodings(self, df):\n",
    "        \"\"\"Add sine and cosine transformations for cyclical features. Handles NaN.\"\"\"\n",
    "        print(\"     Adding cyclic encodings...\")\n",
    "        # Comment: Day encoding uses max_val=31, which is an approximation for month lengths.\n",
    "        for col, max_val in [('Hour', 24), ('DayOfWeek', 7), ('Month', 12), ('Day', 31)]:\n",
    "             # Calculate only for non-NaN values\n",
    "             valid_idx = df[col].notna()\n",
    "             # Use np.sin/cos which handle NaN correctly if input is float\n",
    "             float_col = df.loc[valid_idx, col].astype(float)\n",
    "             df.loc[valid_idx, f'{col}_sin'] = np.sin(2 * np.pi * float_col / max_val)\n",
    "             df.loc[valid_idx, f'{col}_cos'] = np.cos(2 * np.pi * float_col / max_val)\n",
    "             # Fill NaNs potentially created if column didn't exist or after assignment\n",
    "             df[f'{col}_sin'].fillna(0, inplace=True) # Impute with 0 (neutral point for sin/cos)\n",
    "             df[f'{col}_cos'].fillna(0, inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _add_calendar_features(self, df):\n",
    "        \"\"\"Add calendar-based features. Handles NaT.\"\"\"\n",
    "        print(\"     Adding calendar features...\")\n",
    "        dt_col = df['Order_Datetime']\n",
    "        df['Is_Month_Start'] = dt_col.dt.is_month_start.fillna(False).astype(int)\n",
    "        df['Is_Month_End'] = dt_col.dt.is_month_end.fillna(False).astype(int)\n",
    "        df['Is_Quarter_Start'] = dt_col.dt.is_quarter_start.fillna(False).astype(int)\n",
    "        df['Is_Quarter_End'] = dt_col.dt.is_quarter_end.fillna(False).astype(int)\n",
    "        # days_in_month result can be float if NaT present, handle it\n",
    "        df['Days_In_Month'] = dt_col.dt.days_in_month.fillna(0).astype(int) # Impute 0 for NaT\n",
    "\n",
    "        # Order recency (days from most recent date in the dataset)\n",
    "        # Comment: Calculation assumes processing the entire dataset at once.\n",
    "        # For batch processing, max_date should be pre-calculated globally.\n",
    "        if df['Order_Date'].notna().any():\n",
    "             max_date = df['Order_Date'].max()\n",
    "             # Calculate difference only where Order_Date is not NaT\n",
    "             df['Order_Recency_Days'] = (max_date - df['Order_Date']).dt.days\n",
    "             # Impute NaNs (from NaT Order_Date) with a large value or median/mean recency\n",
    "             median_recency = df['Order_Recency_Days'].median()\n",
    "             print(f\"     Imputing Order_Recency_Days NaN with median: {median_recency:.0f} days\")\n",
    "             df['Order_Recency_Days'].fillna(median_recency, inplace=True)\n",
    "        else:\n",
    "             df['Order_Recency_Days'] = np.nan # Or 0 if no valid dates\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _add_holiday_features(self, df):\n",
    "        \"\"\"Add holiday-related features using 'holidays' library. Handles NaT.\"\"\"\n",
    "        print(\"     Adding holiday features...\")\n",
    "        if df['Order_Date'].notna().any():\n",
    "             try:\n",
    "                 # Determine the actual years present in the data + buffer\n",
    "                 valid_dates = df.loc[df['Order_Date'].notna(), 'Order_Date']\n",
    "                 min_year, max_year = valid_dates.dt.year.min(), valid_dates.dt.year.max()\n",
    "                 # Use years present in data +- 1 year for safety\n",
    "                 years_to_check = list(range(min_year -1, max_year + 2))\n",
    "\n",
    "                 # Assuming 'India' - this might need configuration if data spans multiple countries\n",
    "                 print(f\"     Checking Indian holidays for years: {years_to_check}\")\n",
    "                 # Ensure country code is correct. For India it's 'IN'.\n",
    "                 indian_holidays = holidays.country_holidays('IN', years=years_to_check)\n",
    "\n",
    "                 # Apply holiday check only to non-NaT dates\n",
    "                 date_col = df['Order_Date'].dt.date\n",
    "                 valid_idx = df['Order_Date'].notna()\n",
    "\n",
    "                 df['Is_Holiday'] = 0 # Initialize column\n",
    "                 df.loc[valid_idx, 'Is_Holiday'] = date_col[valid_idx].apply(lambda x: 1 if x in indian_holidays else 0)\n",
    "\n",
    "                 df['Next_Day_Holiday'] = 0 # Initialize column\n",
    "                 # Check date + 1 day, handle potential errors if date is None\n",
    "                 df.loc[valid_idx, 'Next_Day_Holiday'] = date_col[valid_idx].apply(\n",
    "                      lambda x: 1 if pd.notna(x) and (x + timedelta(days=1)) in indian_holidays else 0\n",
    "                 )\n",
    "\n",
    "             except Exception as e:\n",
    "                 print(f\"     Warning: Holiday features could not be added. Error: {e}\")\n",
    "                 df['Is_Holiday'] = 0\n",
    "                 df['Next_Day_Holiday'] = 0\n",
    "        else:\n",
    "             print(\"     Skipping holiday features as Order_Date has no valid dates.\")\n",
    "             df['Is_Holiday'] = 0\n",
    "             df['Next_Day_Holiday'] = 0\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _add_interaction_features(self, df):\n",
    "        \"\"\"Add interaction features between time and other variables. Handles NaN.\"\"\"\n",
    "        print(\"     Adding interaction features...\")\n",
    "        # Check if columns exist before creating interactions\n",
    "        # Result will be NaN if any component is NaN, unless explicitly handled\n",
    "        if 'Is_Holiday' in df.columns and 'Is_Rush_Hour' in df.columns:\n",
    "            df['Holiday_Rush_Hour'] = df['Is_Holiday'] * df['Is_Rush_Hour']\n",
    "            df['Holiday_Rush_Hour'].fillna(0, inplace=True) # Assuming no interaction if info missing\n",
    "\n",
    "        if 'Is_Rush_Hour' in df.columns and 'Is_Weekend' in df.columns:\n",
    "            df['Rush_Hour_Weekend'] = df['Is_Rush_Hour'] * df['Is_Weekend']\n",
    "            df['Rush_Hour_Weekend'].fillna(0, inplace=True) # Assuming no interaction if info missing\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "class DeliveryPersonFeatureTransformer(FeatureTransformer):\n",
    "    \"\"\"Creates features related to the delivery person\"\"\"\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = self._copy_dataframe(df)\n",
    "        print(\" -> Running DeliveryPersonFeatureTransformer...\")\n",
    "\n",
    "        # Age and Ratings should be imputed by MissingValueImputer already\n",
    "\n",
    "        # Create age categories (pd.cut handles NaN in input -> NaN in output)\n",
    "        if 'Delivery_person_Age' in df.columns:\n",
    "            print(\"     Creating Age_Group...\")\n",
    "            df['Age_Group'] = pd.cut(\n",
    "                df['Delivery_person_Age'],\n",
    "                bins=[0, 25, 35, 45, 100], # Example bins\n",
    "                labels=['Young', 'Adult', 'Middle_Aged', 'Senior'],\n",
    "                right=False # [0, 25), [25, 35), ...\n",
    "            )\n",
    "            # Optional: Impute NaN Age_Group with mode or 'Unknown'\n",
    "            if df['Age_Group'].isnull().any():\n",
    "                 mode_age_group = df['Age_Group'].mode()\n",
    "                 if not mode_age_group.empty:\n",
    "                     print(f\"     Imputing Age_Group NaN with mode: '{mode_age_group[0]}'\")\n",
    "                     df['Age_Group'].fillna(mode_age_group[0], inplace=True)\n",
    "                 else:\n",
    "                      print(\"     Warning: Could not determine mode for Age_Group. Leaving NaNs or filling with 'Unknown'.\")\n",
    "                      df['Age_Group'].fillna('Unknown', inplace=True) # Example fallback\n",
    "\n",
    "\n",
    "        # Create rating categories (pd.cut handles NaN in input -> NaN in output)\n",
    "        if 'Delivery_person_Ratings' in df.columns:\n",
    "            print(\"     Creating Rating_Category...\")\n",
    "            df['Rating_Category'] = pd.cut(\n",
    "                df['Delivery_person_Ratings'],\n",
    "                bins=[0, 3.5, 4.0, 4.5, 5.0], # Example bins\n",
    "                labels=['Low', 'Medium', 'High', 'Excellent'],\n",
    "                right=True # (0, 3.5], (3.5, 4.0], ... include upper bound\n",
    "            )\n",
    "            # Optional: Impute NaN Rating_Category with mode or 'Unknown'\n",
    "            if df['Rating_Category'].isnull().any():\n",
    "                 mode_rating_cat = df['Rating_Category'].mode()\n",
    "                 if not mode_rating_cat.empty:\n",
    "                     print(f\"     Imputing Rating_Category NaN with mode: '{mode_rating_cat[0]}'\")\n",
    "                     df['Rating_Category'].fillna(mode_rating_cat[0], inplace=True)\n",
    "                 else:\n",
    "                     print(\"     Warning: Could not determine mode for Rating_Category. Leaving NaNs or filling with 'Unknown'.\")\n",
    "                     df['Rating_Category'].fillna('Unknown', inplace=True) # Example fallback\n",
    "\n",
    "        # Extract ID information (ensure column exists)\n",
    "        if 'Delivery_person_ID' in df.columns:\n",
    "            print(\"     Extracting City_Code and Person_Number from ID...\")\n",
    "            # Use regex to capture letters and numbers robustly\n",
    "            extracted_parts = df['Delivery_person_ID'].str.extract(r'([A-Z]+)(\\d+)', expand=True)\n",
    "            if extracted_parts is not None and not extracted_parts.empty:\n",
    "                df['City_Code'] = extracted_parts[0] # First group (letters)\n",
    "                df['Person_Number'] = pd.to_numeric(extracted_parts[1], errors='coerce') # Second group (numbers)\n",
    "                # Optional: Impute NaN Person_Number if needed (e.g., with 0 or median)\n",
    "                df['Person_Number'].fillna(0, inplace=True) # Example: fill with 0\n",
    "            else:\n",
    "                print(\"     Warning: Could not extract City_Code/Person_Number from Delivery_person_ID.\")\n",
    "                df['City_Code'] = 'UNKNOWN'\n",
    "                df['Person_Number'] = 0\n",
    "\n",
    "\n",
    "        # Vehicle condition (ensure column exists and is numeric)\n",
    "        if 'Vehicle_condition' in df.columns and pd.api.types.is_numeric_dtype(df['Vehicle_condition']):\n",
    "             print(\"     Creating Good_Vehicle indicator...\")\n",
    "             # Assuming 0=Bad, 1=Okay, 2=Good, 3=Excellent? Let's say >= 1 is not explicitly bad.\n",
    "             # Adjust threshold based on actual meaning. Using >= 1 as 'not worst'.\n",
    "             df['Good_Vehicle'] = (df['Vehicle_condition'] >= 1).astype(int)\n",
    "             df['Good_Vehicle'].fillna(0, inplace=True) # Assume NaN means not good\n",
    "\n",
    "        print(\" -> DeliveryPersonFeatureTransformer finished.\")\n",
    "        return df\n",
    "\n",
    "\n",
    "class OrderFeatureTransformer(FeatureTransformer):\n",
    "    \"\"\"Creates features related to the order characteristics using One-Hot Encoding\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize encoders - will be fitted during the first transform call\n",
    "        # Note: This stateful approach is better but deviates from the original stateless design.\n",
    "        # For now, we stick to pd.get_dummies with a warning.\n",
    "        pass\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = self._copy_dataframe(df)\n",
    "        print(\" -> Running OrderFeatureTransformer...\")\n",
    "\n",
    "        # Multiple deliveries (already imputed and integer)\n",
    "        if 'multiple_deliveries' in df.columns:\n",
    "            print(\"     Creating Has_Multiple_Deliveries indicator...\")\n",
    "            df['Has_Multiple_Deliveries'] = (df['multiple_deliveries'] > 0).astype(int)\n",
    "\n",
    "        # Road traffic density (already imputed by mode)\n",
    "        if 'Road_traffic_density' in df.columns:\n",
    "            print(\"     Encoding Road_traffic_density...\")\n",
    "            # Ordinal mapping\n",
    "            traffic_mapping = {'Low': 0, 'Medium': 1, 'High': 2, 'Jam': 3}\n",
    "             # Apply mapping, fill any potential remaining NaNs (e.g., if mode failed) with a default like Medium=1\n",
    "            df['Road_traffic_density_Encoded'] = df['Road_traffic_density'].map(traffic_mapping).fillna(1).astype(int)\n",
    "\n",
    "\n",
    "        # === One-Hot Encoding Section ===\n",
    "        # WARNING: Using pd.get_dummies directly can cause issues between train/test sets\n",
    "        # if categories differ. For robust ML, fit sklearn.preprocessing.OneHotEncoder\n",
    "        # on the training data and use it to transform train and test sets.\n",
    "        print(\"     Applying One-Hot Encoding (using pd.get_dummies - see warning in code)...\")\n",
    "\n",
    "        categorical_cols = []\n",
    "        if 'Weatherconditions' in df.columns: categorical_cols.append('Weatherconditions')\n",
    "        if 'Type_of_order' in df.columns: categorical_cols.append('Type_of_order')\n",
    "        if 'Type_of_vehicle' in df.columns: categorical_cols.append('Type_of_vehicle')\n",
    "        if 'City' in df.columns: categorical_cols.append('City')\n",
    "        # Also include derived categorical features if they exist\n",
    "        if 'Time_Period' in df.columns: categorical_cols.append('Time_Period')\n",
    "        if 'Age_Group' in df.columns: categorical_cols.append('Age_Group')\n",
    "        if 'Rating_Category' in df.columns: categorical_cols.append('Rating_Category')\n",
    "        # Add 'City_Code' if extracted and deemed useful as categorical\n",
    "        if 'City_Code' in df.columns: categorical_cols.append('City_Code')\n",
    "\n",
    "\n",
    "        if categorical_cols:\n",
    "             df = pd.get_dummies(df, columns=categorical_cols,\n",
    "                                 prefix=categorical_cols, # Use col name as prefix\n",
    "                                 prefix_sep='_',\n",
    "                                 dummy_na=False, # Don't create explicit NaN column by default\n",
    "                                 drop_first=False) # Keep all categories for now\n",
    "\n",
    "\n",
    "        # Festival (already cleaned to 0/1 indicator 'Festival_Indicator')\n",
    "        # Rename for clarity if desired\n",
    "        if 'Festival_Indicator' in df.columns:\n",
    "            df.rename(columns={'Festival_Indicator': 'Festival_Binary'}, inplace=True)\n",
    "            # Ensure it's int\n",
    "            df['Festival_Binary'] = df['Festival_Binary'].astype(int)\n",
    "\n",
    "\n",
    "        # Create interactions\n",
    "        print(\"     Creating interaction features...\")\n",
    "        if 'Distance_km' in df.columns and 'multiple_deliveries' in df.columns:\n",
    "            df['Multiple_Deliveries_Distance'] = df['multiple_deliveries'] * df['Distance_km']\n",
    "            df['Multiple_Deliveries_Distance'].fillna(0, inplace=True) # Impute interaction NaN\n",
    "\n",
    "        if 'Festival_Binary' in df.columns and 'Road_traffic_density_Encoded' in df.columns:\n",
    "            df['Festival_Traffic'] = df['Festival_Binary'] * df['Road_traffic_density_Encoded']\n",
    "            df['Festival_Traffic'].fillna(0, inplace=True) # Impute interaction NaN\n",
    "\n",
    "        print(\" -> OrderFeatureTransformer finished.\")\n",
    "        return df\n",
    "\n",
    "\n",
    "class FeatureEngineeringPipeline:\n",
    "    \"\"\"Orchestrates the complete feature engineering process\"\"\"\n",
    "\n",
    "    def __init__(self, transformers=None):\n",
    "        # Define the order of transformers carefully\n",
    "        self.transformers = transformers or [\n",
    "            DataCleaner(),\n",
    "            MissingValueImputer(), # Impute raw features early\n",
    "            GeospatialFeatureTransformer(), # Needs cleaned lat/lon\n",
    "            TimeFeatureTransformer(), # Needs cleaned date/time, imputes derived time features\n",
    "            DeliveryPersonFeatureTransformer(), # Needs imputed age/ratings, imputes derived categories\n",
    "            OrderFeatureTransformer() # Needs imputed traffic, creates dummies and interactions\n",
    "            # Add more transformers here if needed (e.g., advanced scaling, final imputation)\n",
    "        ]\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"Apply all feature transformations in sequence\"\"\"\n",
    "        print(\"Starting feature engineering pipeline...\")\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "             raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "\n",
    "        result_df = df # Start with the original df\n",
    "\n",
    "        for transformer in self.transformers:\n",
    "            transformer_name = transformer.__class__.__name__\n",
    "            print(f\"\\nApplying {transformer_name}...\")\n",
    "            try:\n",
    "                result_df = transformer.transform(result_df)\n",
    "                print(f\"Output shape after {transformer_name}: {result_df.shape}\")\n",
    "                 # Optional: print null counts after each step for debugging\n",
    "                 # print(f\"Null counts after {transformer_name}:\\n{result_df.isnull().sum()[result_df.isnull().sum() > 0]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"!!! Error applying {transformer_name}: {e}\")\n",
    "                # Depending on desired behavior, either raise e, or log and continue, or return partial result\n",
    "                raise e # Stop pipeline on error by default\n",
    "\n",
    "        print(\"\\nFeature engineering pipeline completed!\")\n",
    "        # Final check for NaNs\n",
    "        final_nans = result_df.isnull().sum()\n",
    "        print(\"Final NaN counts per column:\")\n",
    "        print(final_nans[final_nans > 0])\n",
    "        if final_nans.sum() > 0:\n",
    "             print(\"\\nWarning: The final DataFrame contains NaN values.\")\n",
    "             print(\"Further imputation or specific model handling might be required.\")\n",
    "\n",
    "        return result_df\n",
    "\n",
    "\n",
    "# Example usage function\n",
    "def process_dataset(input_path, output_path=None):\n",
    "    \"\"\"Load, process a dataset through the feature engineering pipeline, and optionally save.\"\"\"\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Processing dataset from: {input_path}\")\n",
    "    print(\"-\" * 50)\n",
    "    try:\n",
    "        # Load data - consider specifying dtypes for efficiency if known\n",
    "        data = pd.read_csv(input_path)\n",
    "        print(f\"Initial data shape: {data.shape}\")\n",
    "        print(\"Initial null counts:\")\n",
    "        print(data.isnull().sum()[data.isnull().sum() > 0]) # Show only columns with NaNs\n",
    "\n",
    "        # Initialize and run the pipeline\n",
    "        pipeline = FeatureEngineeringPipeline()\n",
    "        processed_data = pipeline.transform(data)\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Processed data shape: {processed_data.shape}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        if output_path:\n",
    "            print(f\"Saving processed data to: {output_path}\")\n",
    "            processed_data.to_csv(output_path, index=False)\n",
    "            print(\"Save complete.\")\n",
    "\n",
    "        return processed_data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file not found at {input_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during dataset processing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print detailed traceback for debugging\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Processing dataset from: C:\\Users\\91786\\Desktop\\DS\\food_ds\\data\\extracted\\train.csv\n",
      "--------------------------------------------------\n",
      "Initial data shape: (45593, 20)\n",
      "Initial null counts:\n",
      "Series([], dtype: int64)\n",
      "Starting feature engineering pipeline...\n",
      "\n",
      "Applying DataCleaner...\n",
      " -> Running DataCleaner...\n",
      "     Cleaning Time_taken(min)...\n",
      "     Cleaning Weatherconditions prefix...\n",
      "     Cleaning Festival column...\n",
      "     Standardizing time strings (Time_Orderd, Time_Order_picked)...\n",
      " -> DataCleaner finished.\n",
      "Output shape after DataCleaner: (45593, 21)\n",
      "\n",
      "Applying MissingValueImputer...\n",
      " -> Running MissingValueImputer...\n",
      "     Imputing Delivery_person_Age NaN with median: 30.00\n",
      "     Imputing Delivery_person_Ratings NaN with median: 4.70\n",
      "     Imputing multiple_deliveries NaN with 0 (assuming NaN means none)\n",
      " -> MissingValueImputer finished.\n",
      "Output shape after MissingValueImputer: (45593, 21)\n",
      "\n",
      "Applying GeospatialFeatureTransformer...\n",
      " -> Running GeospatialFeatureTransformer...\n",
      "     Calculating Distance_km...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91786\\AppData\\Local\\Temp\\ipykernel_25492\\1021153564.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Delivery_person_Age'].fillna(median_age, inplace=True)\n",
      "C:\\Users\\91786\\AppData\\Local\\Temp\\ipykernel_25492\\1021153564.py:112: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Delivery_person_Ratings'].fillna(median_rating, inplace=True)\n",
      "C:\\Users\\91786\\AppData\\Local\\Temp\\ipykernel_25492\\1021153564.py:117: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['multiple_deliveries'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> GeospatialFeatureTransformer finished.\n",
      "Output shape after GeospatialFeatureTransformer: (45593, 22)\n",
      "\n",
      "Applying TimeFeatureTransformer...\n",
      " -> Running TimeFeatureTransformer...\n",
      "     Creating datetime objects (Order_Datetime, Pickup_Datetime)...\n",
      "     Extracting time components (Hour, DayOfWeek, etc.)...\n",
      "     Adding time period features...\n",
      "!!! Error applying TimeFeatureTransformer: cannot convert NA to integer\n",
      "An error occurred during dataset processing: cannot convert NA to integer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\91786\\AppData\\Local\\Temp\\ipykernel_25492\\1021153564.py\", line 616, in process_dataset\n",
      "    processed_data = pipeline.transform(data)\n",
      "  File \"C:\\Users\\91786\\AppData\\Local\\Temp\\ipykernel_25492\\1021153564.py\", line 587, in transform\n",
      "    raise e # Stop pipeline on error by default\n",
      "  File \"C:\\Users\\91786\\AppData\\Local\\Temp\\ipykernel_25492\\1021153564.py\", line 580, in transform\n",
      "    result_df = transformer.transform(result_df)\n",
      "  File \"C:\\Users\\91786\\AppData\\Local\\Temp\\ipykernel_25492\\1021153564.py\", line 197, in transform\n",
      "    df = self._add_time_periods(df)\n",
      "  File \"C:\\Users\\91786\\AppData\\Local\\Temp\\ipykernel_25492\\1021153564.py\", line 284, in _add_time_periods\n",
      "    df['Is_Lunch_Rush'] = df['Hour'].between(11, 13, inclusive='both').astype(int)\n",
      "  File \"c:\\Users\\91786\\anaconda3\\envs\\new_env\\lib\\site-packages\\pandas\\core\\generic.py\", line 6643, in astype\n",
      "    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n",
      "  File \"c:\\Users\\91786\\anaconda3\\envs\\new_env\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 430, in astype\n",
      "    return self.apply(\n",
      "  File \"c:\\Users\\91786\\anaconda3\\envs\\new_env\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 363, in apply\n",
      "    applied = getattr(b, f)(**kwargs)\n",
      "  File \"c:\\Users\\91786\\anaconda3\\envs\\new_env\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 758, in astype\n",
      "    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n",
      "  File \"c:\\Users\\91786\\anaconda3\\envs\\new_env\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 237, in astype_array_safe\n",
      "    new_values = astype_array(values, dtype, copy=copy)\n",
      "  File \"c:\\Users\\91786\\anaconda3\\envs\\new_env\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 179, in astype_array\n",
      "    values = values.astype(dtype, copy=copy)\n",
      "  File \"c:\\Users\\91786\\anaconda3\\envs\\new_env\\lib\\site-packages\\pandas\\core\\arrays\\masked.py\", line 586, in astype\n",
      "    raise ValueError(\"cannot convert NA to integer\")\n",
      "ValueError: cannot convert NA to integer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_file = input_path\n",
    "output_file = output_path\n",
    "processed_df = process_dataset(input_file, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
